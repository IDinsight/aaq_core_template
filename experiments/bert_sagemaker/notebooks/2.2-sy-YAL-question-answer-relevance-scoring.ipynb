{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13dea0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "sagemaker_session_bucket = None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50428f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import botocore\n",
    "from datasets.filesystems import S3FileSystem\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "s3 = S3FileSystem()  \n",
    "s3_prefix='experiment/data/automodel_classification_split'\n",
    "\n",
    "training_input_path = f's3://{sess.default_bucket()}/{s3_prefix}/train_with_neg'\n",
    "test_input_path = f's3://{sess.default_bucket()}/{s3_prefix}/test_with_neg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fb5d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_input_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0dbbb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a08210",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db676b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b33c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters = {\n",
    "    'epochs': 5,\n",
    "    'train_batch_size': 32,\n",
    "    'model_name':'distilbert-base-uncased'\n",
    "}\n",
    "resource_tags = [\n",
    "    {\"Key\":'Project', \"Value\": 'praekelt-skoll'}, \n",
    "    {\"Key\":'BillingCode', \"Value\":'praekelt-skoll'}\n",
    "]\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point='train-classification.py',\n",
    "    source_dir='./scripts',\n",
    "    instance_type='ml.g4dn.xlarge',\n",
    "    instance_count=1,\n",
    "    role=role,\n",
    "    transformers_version='4.12',\n",
    "    pytorch_version='1.9',\n",
    "    py_version='py38',\n",
    "    hyperparameters=hyperparameters,\n",
    "    tags=resource_tags\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a940c4d",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# starting the train job with our uploaded datasets as input\n",
    "huggingface_estimator.fit({'train': training_input_path, 'test': test_input_path})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678da030",
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_estimator.model_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0c946f",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aeb12ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "resource_tags = [\n",
    "    {\"Key\":'Project', \"Value\": 'praekelt-skoll'}, \n",
    "    {\"Key\":'BillingCode', \"Value\":'praekelt-skoll'}\n",
    "]\n",
    "predictor = huggingface_estimator.deploy(1, instance_type='ml.m5.xlarge', tags=resource_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00ba84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "from datasets.filesystems import S3FileSystem\n",
    "\n",
    "# create S3FileSystem without credentials\n",
    "s3 = S3FileSystem()  \n",
    "\n",
    "# load encoded_dataset to from s3 bucket\n",
    "s3_prefix='experiment/data/automodel_classification_split'\n",
    "\n",
    "simple_test_input_path = f's3://{sess.default_bucket()}/{s3_prefix}/test_with_neg'\n",
    "simple_tokenized_test_dataset = load_from_disk(simple_test_input_path, fs=s3)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfda68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_tokenized_test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92ad7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load encoded_dataset to from s3 bucket\n",
    "s3_prefix='experiment/data/untokenized_split'\n",
    "\n",
    "# save test_dataset to s3\n",
    "untokenized_test_input_path = f's3://{sess.default_bucket()}/{s3_prefix}/test_with_neg'\n",
    "untokenized_test_dataset = load_from_disk(untokenized_test_input_path, fs=s3)  \n",
    "untokenized_train_input_path = f's3://{sess.default_bucket()}/{s3_prefix}/train_with_neg'\n",
    "untokenized_train_dataset = load_from_disk(untokenized_train_input_path, fs=s3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac733c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_questions = pd.Series(untokenized_test_dataset['question'])\n",
    "test_questions[test_questions.isin(untokenized_train_dataset['question'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7073bf40",
   "metadata": {},
   "outputs": [],
   "source": [
    "untokenized_test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b85bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "untokenized_inputs = [\n",
    "    {'inputs': ['[CLS] ' + example['question'] + ' [SEP] ' + example['faq_content_to_send'] + ' [SEP]']} for example in untokenized_test_dataset\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52827e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = predictor.predict(untokenized_inputs[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c00d85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7aa502d",
   "metadata": {},
   "outputs": [],
   "source": [
    "untokenized_test_dataset[12]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17baab3",
   "metadata": {},
   "source": [
    "# FAQ Ranking\n",
    "\n",
    "\n",
    "## Batch transform\n",
    "\n",
    "\n",
    "We CAN'T have the same questions in training as in test data.\n",
    "We should select negative samples from only within training data!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d83d4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "faqs = pd.read_csv(\"s3://praekelt-static-resources/experiment/data/yal_faqmatches.csv\")\n",
    "faqs = faqs[~faqs.faq_title.duplicated()]\n",
    "train_df = pd.read_csv(\"s3://praekelt-static-resources/experiment/data/train_dataset_untokenized.csv\")\n",
    "\n",
    "train_faqs = faqs[faqs.faq_id.isin(train_df.faq_id.unique())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c44d892",
   "metadata": {},
   "outputs": [],
   "source": [
    "qdf.faq_id.isin(train_faqs.faq_id.unique()).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07c4152",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_df_list = []\n",
    "faqs_keep_cols = ['faq_id', 'faq_title', 'faq_content_to_send']\n",
    "for question, qdf in untokenized_test_dataset.to_pandas().groupby('question'):\n",
    "    if qdf.shape[0] > 1:\n",
    "        print(f\"Following question occurred multiple times and hence will be skipped: {question}\")\n",
    "        continue\n",
    "    \n",
    "    faq_id = qdf.faq_id.iloc[0]\n",
    "    other_faqs = train_faqs.loc[train_faqs.faq_id != faq_id, faqs_keep_cols].copy()\n",
    "    other_faqs[\"question\"] = question\n",
    "    other_faqs[\"label\"] = 0\n",
    "    \n",
    "    batch_df_list.extend([qdf.drop(columns=['__index_level_0__']), other_faqs])\n",
    "    \n",
    "batch_df = pd.concat(batch_df_list, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e05f1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41c4159",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_df.question.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d125a34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_inputs = batch_df.apply(lambda example: '[CLS] ' + example.question + ' [SEP] ' + example.faq_content_to_send + ' [SEP]', axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9065261e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "from sagemaker.s3 import S3Uploader,s3_path_join\n",
    "\n",
    "# datset files\n",
    "dataset_jsonl_file=\"batch_test_data.jsonl\"\n",
    "\n",
    "with open(dataset_jsonl_file, \"w+\") as outfile:\n",
    "    for text in batch_inputs:\n",
    "        input_dict = {'inputs': text.replace(\"@\",\"\")}\n",
    "        json.dump(input_dict, outfile)\n",
    "        outfile.write('\\n')\n",
    "\n",
    "# uploads a given file to S3.\n",
    "input_s3_path = s3_path_join(\"s3://\",sess.default_bucket(),\"batch_transform/input\")\n",
    "output_s3_path = s3_path_join(\"s3://\",sess.default_bucket(),\"batch_transform/output\")\n",
    "s3_file_uri = S3Uploader.upload(dataset_jsonl_file,input_s3_path)\n",
    "\n",
    "print(f\"{dataset_jsonl_file} uploaded to {s3_file_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3499bb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Transformer to run our batch job\n",
    "batch_job = huggingface_estimator.transformer(\n",
    "    instance_count=1,\n",
    "    instance_type='ml.g4dn.xlarge',\n",
    "    output_path=output_s3_path, # we are using the same s3 path to save the output with the input\n",
    "    strategy='MultiRecord',\n",
    "    tags=resource_tags\n",
    ")\n",
    "\n",
    "# starts batch transform job and uses s3 data as input\n",
    "batch_job.transform(\n",
    "    data=s3_file_uri,\n",
    "    content_type='application/json',    \n",
    "    split_type='Line')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c550d002",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sagemaker.s3 import S3Downloader\n",
    "from ast import literal_eval\n",
    "# creating s3 uri for result file -> input file + .out\n",
    "output_file = f\"{dataset_jsonl_file}.out\"\n",
    "output_path = s3_path_join(output_s3_path,output_file)\n",
    "\n",
    "# download file\n",
    "S3Downloader.download(output_path,'.')\n",
    "\n",
    "batch_transform_result = []\n",
    "with open(output_file) as f:\n",
    "    for line in f:\n",
    "        # converts jsonline array to normal array\n",
    "        line = \"[\" + line.replace(\"[\",\"\").replace(\"]\",\",\") + \"]\"\n",
    "        batch_transform_result = literal_eval(line) \n",
    "        \n",
    "# print results \n",
    "print(batch_transform_result[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453e53e4",
   "metadata": {},
   "source": [
    "## Real-time prediction Ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d44405",
   "metadata": {},
   "source": [
    "Real-time prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d2d98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6a29d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa2dcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = huggingface_estimator.deploy(1, instance_type='ml.m5.xlarge', tags=resource_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25dab9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_results = {\n",
    "    'faq_id': [],\n",
    "    'actual': [],\n",
    "    'predicted': [],\n",
    "    'question': [],\n",
    "    'context': [],\n",
    "}\n",
    "for i, (idx, example) in enumerate(batch_df.iterrows()):\n",
    "    prediction = predictor.predict({'inputs': batch_inputs[i]})[0]\n",
    "    score = int(prediction['label'] == 'LABEL_0') * (1 - prediction['score']) + int(prediction['label'] == 'LABEL_1') * prediction['score'] \n",
    "    pred_results['faq_id'].append(example['faq_id'])\n",
    "    pred_results['actual'].append(float(example['label']))\n",
    "    pred_results['predicted'].append(score)\n",
    "    pred_results['question'].append(example['question'])\n",
    "    pred_results['context'].append(example['faq_content_to_send'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae86407",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = pd.DataFrame(pred_results)\n",
    "pred.plot.scatter(x='actual', y='predicted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e78c4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred[(pred[\"question\"] == \"What's the best way to get over a break up?\") & (pred[\"actual\"] == 1.0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52402169",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred[pred.question == \"What does it mean if my HIV count is very low \"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7810d868",
   "metadata": {},
   "source": [
    "Drop this question because it maps to two FAQs in yal dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee18717",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = pred[pred.question != \"What does it mean if my HIV count is very low \"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037fdfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.to_pickle(\"predictions.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b25691",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4436d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.question.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229aa750",
   "metadata": {},
   "source": [
    "Check ranking quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ce4a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "ranking_accuracy = defaultdict(list)\n",
    "top_n = [1, 3, 5, 7, 10]\n",
    "for question, gdf in pred.groupby(\"question\"):\n",
    "    _df = gdf.sort_values(by='predicted', ascending=False)\n",
    "    for n in top_n:\n",
    "        ranking_accuracy[f\"top_{n}\"].append((_df[\"actual\"].iloc[:n] == 1.0).any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea67d2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranking_acc_result = dict()\n",
    "for k, v in ranking_accuracy.items():\n",
    "    ranking_acc_result[k] = pd.Series(v).mean()\n",
    "    \n",
    "print(ranking_acc_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf454d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cd8942",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in ranking_acc_result.items():\n",
    "    print(f\"{k}\\t{v*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a9866a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, _ = roc_curve(pred.actual, pred.predicted)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(\n",
    "    fpr,\n",
    "    tpr,\n",
    "    color=\"darkorange\",\n",
    "    lw=lw,\n",
    "    label=\"ROC curve (area = %0.2f)\" % roc_auc,\n",
    ")\n",
    "plt.plot([0, 1], [0, 1], color=\"navy\", lw=lw, linestyle=\"--\")\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Receiver operating characteristic example\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683fe007",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(pred.actual.astype(int), pred.predicted > 0.5)\n",
    "import seaborn as sns\n",
    "\n",
    "sns.heatmap(cm, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664a95b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1b9557",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.sample().iloc[0].to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de61436b",
   "metadata": {},
   "source": [
    "## Save data to praekelt s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e9fd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_df = pred.question.drop_duplicates().to_frame()\n",
    "questions_df.to_csv(\"s3://praekelt-static-resources/experiment/data/test_questions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3e4f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (pred.question.apply(lambda x: x.strip()) == \"What does it mean if my HIV count is very low\") & (pred.actual == 1.0)\n",
    "print(pred.question[mask].iloc[0])\n",
    "pred[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9766a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.question.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d976af45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01956019",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array_equal(questions_df.question.values, pred.question.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4baafa9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_untokenized_df = untokenized_train_dataset.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bc30ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_untokenized_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31df2c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_untokenized_df.drop(columns=[\"__index_level_0__\"]).to_csv(\"s3://praekelt-static-resources/experiment/data/train_dataset_untokenized.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce09286",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_untokenized_df.groupby('label').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b85511a",
   "metadata": {},
   "outputs": [],
   "source": [
    "1623"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (main, Mar 26 2022, 15:51:15) \n[Clang 13.1.6 (clang-1316.0.21.2)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
