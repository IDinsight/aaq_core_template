{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958002fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "sagemaker_session_bucket = None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f954026c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import calibration_curve, CalibrationDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d37a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = pd.read_pickle(\"predictions.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48b4081",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ea5a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "(pred.predicted > 0.9).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fea3f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "(pred.actual == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5dcaa36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "display = CalibrationDisplay.from_predictions(pred.actual, pred.predicted, n_bins=20, ax=ax)\n",
    "plt.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4e6f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=pred, x='predicted', binrange=(0, 1), bins=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144bbfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.actual.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8366163",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "\n",
    "fpr, tpr, _ = roc_curve(pred.actual, pred.predicted)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(\n",
    "    fpr,\n",
    "    tpr,\n",
    "    color=\"darkorange\",\n",
    "    lw=lw,\n",
    "    label=\"ROC curve (area = %0.2f)\" % roc_auc,\n",
    ")\n",
    "plt.plot([0, 1], [0, 1], color=\"navy\", lw=lw, linestyle=\"--\")\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Receiver operating characteristic example\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc82529",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(pred.actual.astype(int), pred.predicted > 0.5, normalize='true')\n",
    "sns.heatmap(cm, annot=True)\n",
    "plt.title(\"Confusion matrix (recall)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b772d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(pred.actual.astype(int), pred.predicted > 0.5, normalize='pred')\n",
    "sns.heatmap(cm, annot=True)\n",
    "plt.title(\"Confusion matrix (precision)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9389e4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(pred.actual.astype(int), pred.predicted > 0.95, normalize='pred')\n",
    "sns.heatmap(cm, annot=True)\n",
    "plt.title(\"Confusion matrix (precision)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb91a9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"False positive examples\")\n",
    "print(\"=\"*30)\n",
    "for idx, row in pred[(pred.actual == 0.0) & (pred.predicted > 0.5)].sample(3).iterrows():\n",
    "    print(f\"Q: {row.question}\")\n",
    "    print(f\"A: {row.context}\")\n",
    "    print(\"-\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1620a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"False negative examples\")\n",
    "print(\"=\"*30)\n",
    "for idx, row in pred[(pred.actual == 1.0) & (pred.predicted <= 0.5)].sample(3).iterrows():\n",
    "    print(f\"Q: {row.question}\")\n",
    "    print(f\"A: {row.context}\")\n",
    "    print(\"-\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b0e5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"True positive examples\")\n",
    "print(\"=\"*30)\n",
    "for idx, row in pred[(pred.actual == 1.0) & (pred.predicted > 0.5)].sample(3).iterrows():\n",
    "    print(f\"Q: {row.question}\")\n",
    "    print(f\"A: {row.context}\")\n",
    "    print(\"-\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679db38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "((pred.actual == 1.0) & (pred.predicted <= 0.5)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda8398d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"False negative examples\")\n",
    "print(\"=\"*30)\n",
    "for idx, row in pred[(pred.actual == 1.0) & (pred.predicted <= 0.5)].sample(3).iterrows():\n",
    "    print(f\"Q: {row.question}\")\n",
    "    print(f\"A: {row.context}\")\n",
    "    print(\"-\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704aad5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from sklearn.metrics import (\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    brier_score_loss,\n",
    "    log_loss,\n",
    "    roc_auc_score,\n",
    ")\n",
    "\n",
    "scores = defaultdict(list)\n",
    "y_test = pred.actual\n",
    "y_prob = pred.predicted\n",
    "y_pred = (pred.predicted > 0.5).astype(int)\n",
    "scores[\"Classifier\"].append(\"BERT + YAL data\")\n",
    "\n",
    "for metric in [brier_score_loss, log_loss]:\n",
    "    score_name = metric.__name__.replace(\"_\", \" \").replace(\"score\", \"\").capitalize()\n",
    "    scores[score_name].append(metric(y_test, y_prob))\n",
    "\n",
    "for metric in [precision_score, recall_score, f1_score, roc_auc_score]:\n",
    "    score_name = metric.__name__.replace(\"_\", \" \").replace(\"score\", \"\").capitalize()\n",
    "    scores[score_name].append(metric(y_test, y_pred))\n",
    "\n",
    "score_df = pd.DataFrame(scores).set_index(\"Classifier\")\n",
    "score_df.round(decimals=3)\n",
    "\n",
    "score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7a7be6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p38",
   "language": "python",
   "name": "conda_pytorch_p38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
