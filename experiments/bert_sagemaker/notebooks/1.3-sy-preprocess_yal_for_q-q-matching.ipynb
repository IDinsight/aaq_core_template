{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e6f686",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "sagemaker_session_bucket = None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4132ab28",
   "metadata": {},
   "outputs": [],
   "source": [
    "faqs = pd.read_csv(\"s3://praekelt-static-resources/experiment/data/yal_faqmatches.csv\")\n",
    "faqs = faqs[~faqs.faq_title.duplicated()]\n",
    "\n",
    "df = pd.concat(\n",
    "    (pd.read_csv(f\"s3://praekelt-static-resources/yal_validation/yal_validation_questions_batch_{n}.csv\") \n",
    "     for n in [1, 2])\n",
    "    , axis=0\n",
    ").drop(columns=[\"Unnamed: 0\"]).reset_index(drop=True)\n",
    "\n",
    "df = df[~df.question.duplicated()]\n",
    "\n",
    "df_merged = df.merge(faqs, left_on=\"faq_title\", right_on=\"faq_title\", how=\"inner\")\n",
    "df_merged = df_merged.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e901c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479db2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.question.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8c9dc5",
   "metadata": {},
   "source": [
    "ADDING ANOTHER SYNTHETIC QUESTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0c2a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged[df_merged.faq_id == 119].faq_content_to_send.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8103124b",
   "metadata": {},
   "outputs": [],
   "source": [
    "faq_to_add_question = df_merged[df_merged.faq_id == 119].iloc[0]\n",
    "df_merged[df_merged.faq_id == 119]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce5c8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = df_merged.append({\n",
    "    \"question\": \"how to tell my viral load\",\n",
    "    \"faq_title\": faq_to_add_question.faq_title,\n",
    "    \"faq_id\": 119,\n",
    "    \"faq_content_to_send\": faq_to_add_question.faq_content_to_send, \n",
    "},ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a24d62",
   "metadata": {},
   "source": [
    "# Data Split\n",
    "\n",
    "We need 3 question groups\n",
    "1. Questions to use in representing FAQ contents (each content is represented by (q, a) pair(s) -- there could be multiple questions for an answer)\n",
    "2. Questions for training - train to match these questions to 1\n",
    "3. Questions for testing - test how well the model matches these question to 1\n",
    "\n",
    "Negative samples: for each FAQ, pick 8~10 unrelated questions and label as 0. These questions can come from 1&2 combined.\n",
    "\n",
    "* For Q-Q matching\n",
    "  * Train: positive + negative samples\n",
    "    * (+) For each `Q` in 2, create `(Q, q)` pairs labelled 1 if `Q` and `q` have the same `faq_id` (`q` in 1)\n",
    "    * (-) as described above\n",
    "  * Test:\n",
    "    * (+) For each `Q` in 3, create `(Q, q)` pairs labelled 1 if `Q` and `q` have the same `faq_id` (`q` in 1)\n",
    "    * (-) For each `Q` in 3, create `(Q, q-)` pairs labelled 0, for each `q- != q` in 1\n",
    "* For Q-A relevance scoring\n",
    "    - Train: Use 1 & 2 & negative samples\n",
    "    - Test: 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45eb23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "keep_columns = ['question', 'faq_title', 'faq_id', 'faq_content_to_send']\n",
    "df_merged = df_merged[keep_columns]\n",
    "\n",
    "df_faq_ref = df_merged.groupby('faq_title').sample(2, replace=False, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe2b107",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_faq_ref.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1738ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1eee925",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_faq_ref.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ede922",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged[~df_merged.index.isin(df_faq_ref.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a1cd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_remaining = df_merged.drop(index=df_faq_ref.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d61d910",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_faq_ref = df_faq_ref.rename(columns={'question': 'question_ref'})\n",
    "df_faq_ref.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be53bdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_train_df, test_df = train_test_split(df_merged_remaining, test_size=0.4, stratify=df_merged_remaining.faq_title, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fa932a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_remaining.groupby('faq_title').size().hist(bins=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4e201b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.groupby(\"faq_id\").size().min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612a5c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.groupby(\"faq_id\").size()[df_merged.groupby(\"faq_id\").size() == 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b49ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_remaining.question.isin(df_faq_ref.question_ref).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ceb63ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.question.isin(positive_train_df.question).any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9b3345",
   "metadata": {},
   "source": [
    "## Positive samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a21f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee580e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create positive q-q pairs\n",
    "positive_train_qq_df = positive_train_df.merge(df_faq_ref)\n",
    "positive_train_qq_df.loc[:, \"label\"] = 1\n",
    "positive_train_qq_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7ee810",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert positive_train_df.shape[0] * 2 == positive_train_qq_df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47811413",
   "metadata": {},
   "source": [
    "## Negative samples\n",
    "\n",
    "!!! Remember to set the labels as integers!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb826190",
   "metadata": {},
   "source": [
    "Create a dataframe of all questions for each FAQ (including reference questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e00e807",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_df = pd.concat([\n",
    "    positive_train_df, \n",
    "    df_faq_ref.rename(columns={'question_ref': 'question'})\n",
    "], axis=0)\n",
    "positive_df.loc[:, \"label\"] = 1\n",
    "assert not positive_df.question.duplicated().any()\n",
    "positive_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957bb2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_df[positive_df.faq_id == 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818fcbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_qq_samples = []\n",
    "for cur_id, _df in positive_df.groupby('faq_id'):\n",
    "    # all questions whose FAQ ID is not this one (wrong questions to this FAQ)\n",
    "    cur_negative_questions = positive_df[positive_df.faq_id != cur_id]\n",
    "    cur_negative_questions.loc[:, 'faq_id'] = cur_id\n",
    "    cur_negative_questions.loc[:, 'faq_title'] = faqs.loc[faqs.faq_id == cur_id, \"faq_title\"].iloc[0]\n",
    "    cur_negative_questions.loc[:, 'faq_content_to_send'] = faqs.loc[faqs.faq_id == cur_id, \"faq_content_to_send\"].iloc[0]\n",
    "    \n",
    "    # merge this FAQ's questions -- creates all possible negative samples for this FAQ\n",
    "    all_possible_negative_samples = cur_negative_questions.merge(\n",
    "        _df.rename(columns={\"question\": \"question_ref\"}) # technically question_ref contains correct questions for this FAQ\n",
    "    )\n",
    "    \n",
    "    # Sample 5 negative samples per reference questions\n",
    "    cur_negative_samples = all_possible_negative_samples.groupby(\"question_ref\").sample(5, random_state=42)\n",
    "    negative_qq_samples.append(cur_negative_samples)\n",
    "    \n",
    "negative_train_qq_df = pd.concat(negative_qq_samples, axis=0)\n",
    "negative_train_qq_df.loc[:, \"label\"] = 0\n",
    "\n",
    "train_df = pd.concat([negative_train_qq_df, positive_train_qq_df], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d181637",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.question_ref.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d53f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_train_df.question.nunique() + df_faq_ref.question_ref.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd10a4f",
   "metadata": {},
   "source": [
    "Create dataset for Q-A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8d347c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get negative q-a pairs, from the negative question-question pairs, get unique rows barring reference questions\n",
    "negative_train_df = negative_train_qq_df.drop(columns=[\"question_ref\"]).drop_duplicates()\n",
    "\n",
    "# combine all positive q-a pairs with negative ones.\n",
    "# Note that we can include the reference questions in the positive samples here.\n",
    "qa_train_df = pd.concat([positive_df, negative_train_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93208cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ffac87",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_test_df = test_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6c2bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"FAQs:\", faqs.shape[0])\n",
    "print(\"Merged FAQs:\", df_merged.faq_id.nunique())\n",
    "print(\"Reference Q-A pairs:\", df_faq_ref.shape[0])\n",
    "print(\"Positive training data:\", positive_df.shape[0])\n",
    "print(\"Negative training data:\", negative_train_df.shape[0])\n",
    "print(\"Positive training Q-Q data:\", positive_train_qq_df.shape[0])\n",
    "print(\"Negative training Q-Q data:\", negative_train_qq_df.shape[0])\n",
    "print(\"Training data for Q-Q:\", train_df.shape[0])\n",
    "print(\"Training data for Q-A:\", qa_train_df.shape[0])\n",
    "print(\"Test data:\", test_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d57df1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.groupby('faq_id').size().hist(bins=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41cba697",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.groupby(\"label\").size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701a98c9",
   "metadata": {},
   "source": [
    "### Test DF for q-q matching\n",
    "\n",
    "* Test:\n",
    "  * (+) For each Q in 3, create (Q, q) pairs labelled 1 if Q and q have the same faq_id (q in 1)\n",
    "  * (-) For each Q in 3, create (Q, q-) pairs labelled 0, for each q- != q in 1\n",
    "\n",
    "Might as well create entire test set.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4025f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_test_df = test_df.copy()\n",
    "positive_test_df.loc[:, \"label\"] = 1\n",
    "positive_test_qq_df = positive_test_df.merge(df_faq_ref)\n",
    "positive_test_qq_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696aedcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not positive_test_df.question.duplicated().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d830d1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_qq_test_samples = []\n",
    "\n",
    "# For each question, get all (q, a) pairs such that a doesn't answer the question\n",
    "for idx, row in positive_test_df.iterrows():\n",
    "    cur_id = row[\"faq_id\"]\n",
    "    other_faqs = df_faq_ref[df_faq_ref.faq_id != cur_id].copy()\n",
    "    other_faqs.loc[:, \"question\"] = row[\"question\"]\n",
    "    negative_qq_test_samples.append(other_faqs)\n",
    "    \n",
    "negative_test_qq_df = pd.concat(negative_qq_test_samples, axis=0)\n",
    "negative_test_qq_df.loc[:, \"label\"] = 0\n",
    "\n",
    "test_df = pd.concat([negative_test_qq_df, positive_test_qq_df],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc095a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_faq_ref.groupby(\"faq_id\").size().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0430490d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.groupby(\"question\").size().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d799ba38",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert test_df.shape[0] == positive_test_df.shape[0] * 2 * df_faq_ref.faq_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999d40f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74686dca",
   "metadata": {},
   "source": [
    "### Test DF for q-a matching\n",
    "\n",
    "just drop duplicates barring question_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f148ea54",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_test_df = test_df.drop(columns=['question_ref']).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234eaa4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703e6501",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert qa_test_df.shape[0] == df_faq_ref.faq_id.nunique() * positive_test_df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a38de89",
   "metadata": {},
   "source": [
    "# Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07855791",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c9650c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965219e0",
   "metadata": {},
   "source": [
    "Training data for question-answer pairing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc293a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_matching_train_dataset = Dataset.from_pandas(qa_train_df)\n",
    "qa_matching_test_dataset = Dataset.from_pandas(qa_test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b4fc95",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4620fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_tokenize_qq(examples):\n",
    "    return tokenizer(\n",
    "        examples['question'], \n",
    "        examples['question_ref'], \n",
    "        max_length=384,\n",
    "        padding='max_length',\n",
    "        truncation=\"only_second\",\n",
    "        return_overflowing_tokens=True,\n",
    "        stride=128,\n",
    "    )\n",
    "\n",
    "def custom_tokenize_qa(examples):\n",
    "    return tokenizer(\n",
    "        examples['question'], \n",
    "        examples['faq_content_to_send'], \n",
    "        max_length=384,\n",
    "        padding='max_length',\n",
    "        truncation=\"only_second\",\n",
    "        return_overflowing_tokens=True,\n",
    "        stride=128,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce82b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_columns = ['question', 'faq_id', 'faq_title', 'faq_content_to_send', '__index_level_0__', 'question_ref', ]\n",
    "\n",
    "simple_tokenized_train_dataset = train_dataset.map(custom_tokenize_qq, batched=True, batch_size=1000, remove_columns=remove_columns)\n",
    "simple_tokenized_test_dataset = test_dataset.map(custom_tokenize_qq, batched=True, batch_size=1000, remove_columns=remove_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7092477",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_columns = ['question', 'faq_id', 'faq_title', 'faq_content_to_send', '__index_level_0__', ]\n",
    "tokenized_qa_matching_train_dataset = qa_matching_train_dataset.map(custom_tokenize_qa, batched=True, batch_size=1000, remove_columns=remove_columns)\n",
    "tokenized_qa_matching_test_dataset = qa_matching_test_dataset.map(custom_tokenize_qa, batched=True, batch_size=1000, remove_columns=remove_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8c28ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import botocore\n",
    "from datasets.filesystems import S3FileSystem\n",
    "\n",
    "s3 = S3FileSystem()\n",
    "s3_bucket = 'praekelt-static-resources'\n",
    "s3_prefix='experiment/data/yal/question-question-matching'\n",
    "\n",
    "# save train_dataset to s3\n",
    "simple_training_input_path = f's3://{s3_bucket}/{s3_prefix}/train'\n",
    "simple_tokenized_train_dataset.save_to_disk(simple_training_input_path,fs=s3)\n",
    "\n",
    "# save test_dataset to s3\n",
    "simple_test_input_path = f's3://{s3_bucket}/{s3_prefix}/test'\n",
    "simple_tokenized_test_dataset.save_to_disk(simple_test_input_path,fs=s3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809bb5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "n_positive = positive_train_qq_df.shape[0]\n",
    "tokenized_qq_matching_test_dataset_short = Dataset.from_dict(simple_tokenized_test_dataset.shuffle().sort(\"label\", reverse=True)[:2*n_positive])\n",
    "\n",
    "# save short test_dataset to s3\n",
    "short_simple_test_input_path = f's3://{s3_bucket}/{s3_prefix}/test_short'\n",
    "tokenized_qq_matching_test_dataset_short.save_to_disk(short_simple_test_input_path,fs=s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8726eaeb",
   "metadata": {},
   "source": [
    "qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dee710",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_prefix='experiment/data/yal/question-answer-matching'\n",
    "\n",
    "# save train_dataset to s3\n",
    "qa_training_input_path = f's3://{s3_bucket}/{s3_prefix}/train'\n",
    "tokenized_qa_matching_train_dataset.save_to_disk(qa_training_input_path,fs=s3)\n",
    "\n",
    "# save test_dataset to s3\n",
    "simple_test_input_path = f's3://{s3_bucket}/{s3_prefix}/test'\n",
    "tokenized_qa_matching_test_dataset.save_to_disk(simple_test_input_path,fs=s3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8d00d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_positive = positive_df.shape[0]\n",
    "tokenized_qa_matching_test_dataset_short = Dataset.from_dict(tokenized_qa_matching_test_dataset.shuffle().sort(\"label\", reverse=True)[:2*n_positive])\n",
    "\n",
    "# save short test_dataset to s3\n",
    "short_simple_test_input_path = f's3://{s3_bucket}/{s3_prefix}/test_short'\n",
    "tokenized_qa_matching_test_dataset_short.save_to_disk(short_simple_test_input_path,fs=s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ab8374",
   "metadata": {},
   "source": [
    "Also save untokenized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887e6738",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_prefix='experiment/data/yal/question-question-matching'\n",
    "\n",
    "# save train_dataset to s3\n",
    "training_input_path = f's3://{s3_bucket}/{s3_prefix}/train_untokenized'\n",
    "train_dataset.save_to_disk(training_input_path,fs=s3)\n",
    "\n",
    "# save test_dataset to s3\n",
    "test_input_path = f's3://{s3_bucket}/{s3_prefix}/test_untokenized'\n",
    "test_dataset.save_to_disk(test_input_path,fs=s3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b36c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_prefix='experiment/data/yal/question-answer-matching'\n",
    "\n",
    "# save train_dataset to s3\n",
    "training_input_path = f's3://{s3_bucket}/{s3_prefix}/train_untokenized'\n",
    "qa_matching_train_dataset.save_to_disk(training_input_path,fs=s3)\n",
    "\n",
    "# save test_dataset to s3\n",
    "test_input_path = f's3://{s3_bucket}/{s3_prefix}/test_untokenized'\n",
    "qa_matching_test_dataset.save_to_disk(test_input_path,fs=s3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p38",
   "language": "python",
   "name": "conda_pytorch_p38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
