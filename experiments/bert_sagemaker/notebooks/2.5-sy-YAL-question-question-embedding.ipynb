{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80be949a",
   "metadata": {},
   "source": [
    "# Question-Question matching using sentence embeddings\n",
    "\n",
    "* Get embeddings for all test questions, `E_test`\n",
    "  * get unique `question` from test data\n",
    "* Get embeddings for all reference questions, `E_ref`\n",
    "  * from training data for q-q, get the positive samples, and extract unique questions from `question_ref` column\n",
    "* Get match scores for `E_text`, `E_ref` all combinations\n",
    "* Pool cosine similarities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7872d908",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "sagemaker_session_bucket = None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb3f8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import botocore\n",
    "from datasets.filesystems import S3FileSystem\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "s3 = S3FileSystem()\n",
    "s3_bucket = 'praekelt-static-resources'\n",
    "s3_prefix='experiment/data/yal/question-question-matching'\n",
    "\n",
    "training_input_path = f's3://{s3_bucket}/{s3_prefix}/train'\n",
    "test_input_path = f's3://{s3_bucket}/{s3_prefix}/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3019c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_input_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90b6ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface.model import HuggingFaceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0fbcc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea3465d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sentences we want to encode. Example:\n",
    "sentence = ['This framework generates embeddings for each input sentence']\n",
    "\n",
    "\n",
    "#Sentences are encoded by calling model.encode()\n",
    "embedding = model.encode(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120624cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d60693",
   "metadata": {},
   "outputs": [],
   "source": [
    "hub = {\n",
    "  'HF_MODEL_ID': 'sentence-transformers/all-MiniLM-L6-v2', # model_id from hf.co/models\n",
    "  'HF_TASK': 'feature-extraction'                           # NLP task you want to use for predictions\n",
    "}\n",
    "\n",
    "\n",
    "huggingface_estimator = HuggingFaceModel(\n",
    "    env=hub,\n",
    "    role=role,\n",
    "    transformers_version='4.12',\n",
    "    pytorch_version='1.9',\n",
    "    py_version='py38',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064e4728",
   "metadata": {},
   "source": [
    "## Realtime inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4356952b",
   "metadata": {},
   "outputs": [],
   "source": [
    "resource_tags = [\n",
    "    {\"Key\":'Project', \"Value\": 'praekelt-skoll'}, \n",
    "    {\"Key\":'BillingCode', \"Value\":'praekelt-skoll'}\n",
    "]\n",
    "\n",
    "predictor = huggingface_estimator.deploy(initial_instance_count=1, instance_type=\"ml.m5.xlarge\", tags=resource_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57160ac",
   "metadata": {},
   "source": [
    "# FAQ Ranking\n",
    "\n",
    "\n",
    "## Batch transform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9540f96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "untokenized_train_input_path = f's3://{s3_bucket}/{s3_prefix}/train_untokenized'\n",
    "untokenized_train_dataset = load_from_disk(untokenized_train_input_path, fs=s3)\n",
    "\n",
    "untokenized_test_input_path = f's3://{s3_bucket}/{s3_prefix}/test_untokenized'\n",
    "untokenized_test_dataset = load_from_disk(untokenized_test_input_path, fs=s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a114c47",
   "metadata": {},
   "source": [
    "Get reference questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacfddec",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = untokenized_train_dataset.to_pandas()\n",
    "df_faq_ref = train_df[train_df.label == 1].drop(columns=[\"question\", \"faq_content_to_send\", \"__index_level_0__\"]).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53318b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_faq_ref"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5e4b59",
   "metadata": {},
   "source": [
    "get test questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643ed089",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = untokenized_test_dataset.to_pandas()\n",
    "test_questions = test_df[~test_df['question'].duplicated()].drop(columns=[\"question_ref\", \"faq_content_to_send\", \"__index_level_0__\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d8a722",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417c348d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_df = pd.concat(\n",
    "    [\n",
    "        df_faq_ref.assign(is_test=0).rename(columns={\"question_ref\": \"question\"}),\n",
    "        test_questions.assign(is_test=1)\n",
    "    ], \n",
    "    axis=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2571fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bba33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_df.question.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29691634",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "whitespace = re.compile('\\s+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc9dbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_inputs = batch_df.apply(\n",
    "    lambda example: \n",
    "    '[CLS] ' + whitespace.sub(' ', example.question) + ' [SEP]', \n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc283998",
   "metadata": {},
   "source": [
    "## Realtime prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff4ae3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_array = []\n",
    "for text in batch_inputs:\n",
    "    data = {\"inputs\": text}\n",
    "    output_array.append(np.asarray(predictor.predict(data)[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc69df0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "output = np.asarray(output_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ad634c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3916493c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_df.loc[:, \"embedding\"] = output_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d63cb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8721bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebd35d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_embeddings = batch_df[batch_df.is_test == 0]\n",
    "q_embeddings = batch_df[batch_df.is_test == 1]\n",
    "\n",
    "ref = np.asarray(ref_embeddings.embedding.tolist())\n",
    "q = np.asarray(q_embeddings.embedding.tolist())\n",
    "\n",
    "ref.shape, q.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6914e411",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_norm = np.linalg.norm(ref, axis=1)\n",
    "cossim_list = []\n",
    "\n",
    "for qi in q:\n",
    "    cossim = np.dot(qi, ref.T) / (np.linalg.norm(qi) * ref_norm)\n",
    "    cossim_list.append(cossim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c802032e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cossim_arr = np.asarray(cossim_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63adc9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cossim_arr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae8caaf",
   "metadata": {},
   "source": [
    "Now we have scores for each reference question.\n",
    "\n",
    "For each question we want to average the scores among same FAQ questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bf9e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_embeddings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bfd070",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "top10_scores = defaultdict(list)\n",
    "ks = [1, 3, 5, 7, 10]\n",
    "\n",
    "for i, a in enumerate(cossim_arr):\n",
    "    ref_embeddings.loc[:, \"cossim\"] = a.flatten()\n",
    "    scores = ref_embeddings.groupby(\"faq_id\").cossim.agg([\"mean\", \"max\"])\n",
    "    top10_by_max = scores[(scores.shape[0] - scores['max'].argsort()) <= 10].index.tolist()\n",
    "    top10_by_mean = scores[(scores.shape[0] - scores['mean'].argsort()) <= 10].index.tolist()\n",
    "    top10_scores['question'].append(q_embeddings.iloc[i].question)\n",
    "    top10_scores['top10_by_max'].append(top10_by_max)\n",
    "    top10_scores['top10_by_mean'].append(top10_by_mean)\n",
    "    top10_scores['faq_id'].append(q_embeddings.iloc[i].faq_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e595a08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "top10_score_df = pd.DataFrame(top10_scores)\n",
    "\n",
    "accuracy_by_max = defaultdict(list)\n",
    "accuracy_by_mean = defaultdict(list)\n",
    "\n",
    "for k in ks:\n",
    "    accuracy_by_max[k].append(top10_score_df.apply(lambda row: row.faq_id in row.top10_by_max[:k], axis=1).mean())\n",
    "    accuracy_by_mean[k].append(top10_score_df.apply(lambda row: row.faq_id in row.top10_by_mean[:k], axis=1).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314bbb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_by_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19ca832",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa754c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cossim_argsort = np.argsort(cossim_arr)  # rank of each number, smallest is 0\n",
    "cossim_argsort_argsort = np.argsort(cossim_argsort) # rank of position, top 10 is the last 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5769736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_results = {\n",
    "#     'faq_id': [],\n",
    "#     'actual': [],\n",
    "#     'predicted': [],\n",
    "#     'question': [],\n",
    "#     'question_ref': [],\n",
    "#     'context': [],\n",
    "# }\n",
    "\n",
    "# for i, prediction in enumerate(batch_transform_result):\n",
    "#     score = int(prediction['label'] == 'LABEL_0') * (1 - prediction['score']) + int(prediction['label'] == 'LABEL_1') * prediction['score']\n",
    "#     example = batch_df.iloc[i]\n",
    "#     pred_results['faq_id'].append(example['faq_id'])\n",
    "#     pred_results['actual'].append(float(example['label']))\n",
    "#     pred_results['predicted'].append(score)\n",
    "#     pred_results['question'].append(example['question'])\n",
    "#     pred_results['question_ref'].append(example['question_ref'])\n",
    "#     pred_results['context'].append(example['faq_content_to_send'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbec2814",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = pd.DataFrame(pred_results)\n",
    "pred.to_pickle(s3_path_join(output_s3_path,'predictions_question_embedding.pkl')\n",
    "pred.plot.scatter(x='actual', y='predicted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ad92ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.question.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9680a9c5",
   "metadata": {},
   "source": [
    "Check ranking quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706a0114",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "ranking_accuracy = defaultdict(list)\n",
    "top_n = [1, 3, 5, 7, 10]\n",
    "for question, gdf in pred.groupby(\"question\"):\n",
    "    _df = gdf.sort_values(by='predicted', ascending=False)\n",
    "    for n in top_n:\n",
    "        ranking_accuracy[f\"top_{n}\"].append((_df[\"actual\"].iloc[:n] == 1.0).any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070ec122",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranking_acc_result = dict()\n",
    "for k, v in ranking_accuracy.items():\n",
    "    ranking_acc_result[k] = pd.Series(v).mean()\n",
    "    \n",
    "print(ranking_acc_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eead0ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "(pd.Series(ranking_acc_result) * 100).to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48984fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5666d82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in ranking_acc_result.items():\n",
    "    print(f\"{k}\\t{v*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320bdbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, _ = roc_curve(pred.actual, pred.predicted)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(\n",
    "    fpr,\n",
    "    tpr,\n",
    "    color=\"darkorange\",\n",
    "    lw=lw,\n",
    "    label=\"ROC curve (area = %0.2f)\" % roc_auc,\n",
    ")\n",
    "plt.plot([0, 1], [0, 1], color=\"navy\", lw=lw, linestyle=\"--\")\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Receiver operating characteristic example\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b309a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(pred.actual.astype(int), pred.predicted > 0.5)\n",
    "import seaborn as sns\n",
    "\n",
    "sns.heatmap(cm, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781d35b8",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p38",
   "language": "python",
   "name": "conda_pytorch_p38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
