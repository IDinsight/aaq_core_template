{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32824523",
   "metadata": {},
   "source": [
    "# Ensemble QQ and QA results\n",
    "\n",
    "1. Train a QA matching model with the same train-test split (see 1.3 for more details)\n",
    "2. Combine QQ and QA model scores using various methods (see below)\n",
    "\n",
    "## Potential methods\n",
    "Given a query $q'$ we want to find top 5 $(q_{ij}, a_i)$ pairs. \n",
    "\n",
    "An FAQ content (document) $a_i$ can have many questions $q_{ij}, j=1, \\dots, k_i$ associated with it.\n",
    "\n",
    "### Method 1\n",
    "1. Given $q'$, compute scores for all $a_i$'s using Q-A scorer\n",
    "2. From top $N$ $a_i$'s, say $A_N$, get corresponding questions $Q_N = \\{q_{ij}| \\forall j \\text{ and } i \\text{ s.t. } a_i \\in A_N\\}$\n",
    "3. Compute scores for each pair $(q', q_{ij}), q_{ij}\\in Q_N$, score $s_{ij}$ using Q-Q scorer\n",
    "4. $s_i = \\max_j(s_{ij})$\n",
    "5. Rank $a_i$'s by $s_i$'s\n",
    "\n",
    "Why would this work?\n",
    "- we narrow the pool of candidates using FAQ contents (num(FAQ contents) <= num(FAQ content quesitons))\n",
    "- if there are questions that are very similar, we'll get a high score\n",
    "\n",
    "### Method 2\n",
    "1. Compute scores for all $(q', a_i)$ (using Q-A)\n",
    "2. Compute scores for all $(q', q_{ij})$ (using Q-Q)\n",
    "3. Pool scores for each $a_i$ and rank\n",
    "\n",
    "Pooling methods:\n",
    "* Average\n",
    "* Max\n",
    "\n",
    "### Method 3\n",
    "Method 1 but use Q-Q scoring first, then Q-A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0dda05",
   "metadata": {},
   "source": [
    "# 1. Train Q-A matching model\n",
    "\n",
    "## 1.1 Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf31f75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "sagemaker_session_bucket = None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ec1ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import botocore\n",
    "from datasets.filesystems import S3FileSystem\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "s3 = S3FileSystem()  \n",
    "s3_prefix='experiment/data/yal/question-answer-matching'\n",
    "s3_bucket = 'praekelt-static-resources'\n",
    "\n",
    "training_input_path = f's3://{s3_bucket}/{s3_prefix}/train'\n",
    "test_short_input_path = f's3://{s3_bucket}/{s3_prefix}/test_short'\n",
    "test_full_input_path = f's3://{s3_bucket}/{s3_prefix}/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0ca706",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters = {\n",
    "    'epochs': 5,\n",
    "    'train_batch_size': 32,\n",
    "    'model_name':'distilbert-base-uncased'\n",
    "}\n",
    "\n",
    "resource_tags = [\n",
    "    {\"Key\":'Project', \"Value\": 'praekelt-skoll'}, \n",
    "    {\"Key\":'BillingCode', \"Value\":'praekelt-skoll'},\n",
    "    {\"Key\": 'model_type', \"Value\": 'question-answer-pair-score'}\n",
    "]\n",
    "\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point='train-classification.py',\n",
    "    source_dir='./scripts',\n",
    "    instance_type='ml.g4dn.xlarge',\n",
    "    instance_count=1,\n",
    "    role=role,\n",
    "    transformers_version='4.12',\n",
    "    pytorch_version='1.9',\n",
    "    py_version='py38',\n",
    "    hyperparameters=hyperparameters,\n",
    "    tags=resource_tags\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4575a671",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# starting the train job with our uploaded datasets as input\n",
    "huggingface_estimator.fit({'train': training_input_path, 'test': test_short_input_path})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04362afe",
   "metadata": {},
   "source": [
    "## 1.2 Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6da62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "# untokenized_train_input_path = f's3://{s3_bucket}/{s3_prefix}/train_untokenized'\n",
    "# untokenized_train_dataset = load_from_disk(untokenized_train_input_path, fs=s3)\n",
    "\n",
    "untokenized_test_input_path = f's3://{s3_bucket}/{s3_prefix}/test_untokenized'\n",
    "untokenized_test_dataset = load_from_disk(untokenized_test_input_path, fs=s3)\n",
    "\n",
    "batch_df = untokenized_test_dataset.to_pandas()\n",
    "batch_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6e0c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "whitespace = re.compile('\\s+')\n",
    "\n",
    "batch_inputs = batch_df.apply(\n",
    "    lambda example: \n",
    "    '[CLS] ' + whitespace.sub(' ', example.question) + ' [SEP] ' + whitespace.sub(' ', example.faq_content_to_send) + ' [SEP]', \n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bea2544",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "from sagemaker.s3 import S3Uploader,s3_path_join\n",
    "\n",
    "# datset files\n",
    "dataset_jsonl_file = \"question_answer_pair_score.jsonl\"\n",
    "\n",
    "with open(dataset_jsonl_file, \"w+\") as outfile:\n",
    "    for text in batch_inputs.tolist():\n",
    "        input_dict = {'inputs': text.replace(\"@\",\"\")}\n",
    "        json.dump(input_dict, outfile)\n",
    "        outfile.write('\\n')\n",
    "\n",
    "# uploads a given file to S3.\n",
    "batch_transform_s3_prefix = 's3://praekelt-static-resources/experiment/outputs/batch-transform'\n",
    "input_s3_path = s3_path_join(batch_transform_s3_prefix,\"input.jsonl\")\n",
    "output_s3_path = s3_path_join(batch_transform_s3_prefix,\"output\")\n",
    "s3_file_uri = S3Uploader.upload(dataset_jsonl_file,input_s3_path)\n",
    "\n",
    "print(f\"{dataset_jsonl_file} uploaded to {s3_file_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd2164f",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# create Transformer to run our batch job\n",
    "batch_job = huggingface_estimator.transformer(\n",
    "    instance_count=1,\n",
    "    instance_type='ml.g4dn.2xlarge',\n",
    "    output_path=output_s3_path, # we are using the same s3 path to save the output with the input\n",
    "    strategy='SingleRecord',\n",
    "    tags=resource_tags,\n",
    ")\n",
    "\n",
    "# starts batch transform job and uses s3 data as input\n",
    "batch_job.transform(\n",
    "    data=s3_file_uri,\n",
    "    content_type='application/json',    \n",
    "    split_type='Line'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be94f678",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_jsonl_file = 'question_answer_pair_score.jsonl'\n",
    "output_file = f\"{dataset_jsonl_file}.out\"\n",
    "output_path = s3_path_join('s3://praekelt-static-resources/experiment/outputs/batch-transform/output', output_file)\n",
    "\n",
    "# download file\n",
    "S3Downloader.download(output_path,'.')\n",
    "\n",
    "batch_transform_result = []\n",
    "with open(output_file) as f:\n",
    "    for line in f:\n",
    "        # converts jsonline array to normal array\n",
    "        line = \"[\" + line.replace(\"[\",\"\").replace(\"]\",\",\") + \"]\"\n",
    "        batch_transform_result = literal_eval(line) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306e67e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_results = {\n",
    "    'faq_id': [],\n",
    "    'actual': [],\n",
    "    'predicted': [],\n",
    "    'question': [],\n",
    "    'context': [],\n",
    "}\n",
    "\n",
    "for i, prediction in enumerate(batch_transform_result):\n",
    "    score = int(prediction['label'] == 'LABEL_0') * (1 - prediction['score']) + int(prediction['label'] == 'LABEL_1') * prediction['score']\n",
    "    example = batch_df.iloc[i]\n",
    "    pred_results['faq_id'].append(example['faq_id'])\n",
    "    pred_results['actual'].append(float(example['label']))\n",
    "    pred_results['predicted'].append(score)\n",
    "    pred_results['question'].append(example['question'])\n",
    "    pred_results['context'].append(example['faq_content_to_send'])\n",
    "    \n",
    "pred = pd.DataFrame(pred_results)\n",
    "pred.to_pickle(s3_path_join(output_s3_path,'predictions_question_answer_pair_score.pkl'))\n",
    "pred.plot.scatter(x='actual', y='predicted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f3cb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "ranking_accuracy = defaultdict(list)\n",
    "top_n = [1, 3, 5, 7, 10]\n",
    "for question, gdf in pred.groupby(\"question\"):\n",
    "    _df = gdf.sort_values(by='predicted', ascending=False)\n",
    "    for n in top_n:\n",
    "        ranking_accuracy[f\"top_{n}\"].append((_df[\"actual\"].iloc[:n] == 1.0).any())\n",
    "        \n",
    "ranking_acc_result = dict()\n",
    "for k, v in ranking_accuracy.items():\n",
    "    ranking_acc_result[k] = pd.Series(v).mean()\n",
    "    \n",
    "(pd.Series(ranking_acc_result) * 100).to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f7cd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fpr, tpr, _ = roc_curve(pred.actual, pred.predicted)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(\n",
    "    fpr,\n",
    "    tpr,\n",
    "    color=\"darkorange\",\n",
    "    lw=lw,\n",
    "    label=\"ROC curve (area = %0.2f)\" % roc_auc,\n",
    ")\n",
    "plt.plot([0, 1], [0, 1], color=\"navy\", lw=lw, linestyle=\"--\")\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Receiver operating characteristic example\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0a97c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(pred.actual.astype(int), pred.predicted > 0.5)\n",
    "import seaborn as sns\n",
    "\n",
    "sns.heatmap(cm, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6c94d9",
   "metadata": {},
   "source": [
    "# 2. Load predictions from Q-Q scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74e9b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea87b7b",
   "metadata": {},
   "source": [
    "Load batch_df again.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86215b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_prefix='experiment/data/yal/question-question-matching'\n",
    "untokenized_test_qq_input_path = f's3://{s3_bucket}/{s3_prefix}/test_untokenized'\n",
    "untokenized_test_qq_dataset = load_from_disk(untokenized_test_qq_input_path, fs=s3)\n",
    "\n",
    "batch_qq_df = untokenized_test_qq_dataset.to_pandas()\n",
    "batch_qq_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196e90e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_jsonl_file = 'question_question_pair_score.jsonl'\n",
    "output_file = f\"{dataset_jsonl_file}.out\"\n",
    "output_path = s3_path_join('s3://praekelt-static-resources/experiment/outputs/batch-transform/output', output_file)\n",
    "\n",
    "# download file\n",
    "S3Downloader.download(output_path,'.')\n",
    "\n",
    "batch_transform_result = []\n",
    "with open(output_file) as f:\n",
    "    for line in f:\n",
    "        # converts jsonline array to normal array\n",
    "        line = \"[\" + line.replace(\"[\",\"\").replace(\"]\",\",\") + \"]\"\n",
    "        batch_transform_result = literal_eval(line) \n",
    "\n",
    "pred_results = {\n",
    "    'faq_id': [],\n",
    "    'actual': [],\n",
    "    'predicted': [],\n",
    "    'question': [],\n",
    "    'question_ref': [],\n",
    "    'context': [],\n",
    "}\n",
    "\n",
    "for i, prediction in enumerate(batch_transform_result):\n",
    "    score = int(prediction['label'] == 'LABEL_0') * (1 - prediction['score']) + int(prediction['label'] == 'LABEL_1') * prediction['score']\n",
    "    example = batch_qq_df.iloc[i]\n",
    "    pred_results['faq_id'].append(example['faq_id'])\n",
    "    pred_results['actual'].append(float(example['label']))\n",
    "    pred_results['predicted'].append(score)\n",
    "    pred_results['question'].append(example['question'])\n",
    "    pred_results['question_ref'].append(example['question_ref'])\n",
    "    pred_results['context'].append(example['faq_content_to_send'])\n",
    "    \n",
    "pred_qq = pd.DataFrame(pred_results)\n",
    "pred_qq.to_pickle(s3_path_join(output_s3_path,'predictions_question_question_pair_score.pkl'))\n",
    "pred_qq.plot.scatter(x='actual', y='predicted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e54ff8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get scores for each (q', a_i) pair, by averaging over q_ij's\n",
    "scores_avged = pred_qq.groupby(['question', 'faq_id']).predicted.mean().reset_index().rename(columns={\"predicted\": \"predicted_qq_avg\"})\n",
    "scores_maxed = pred_qq.groupby(['question', 'faq_id']).predicted.max().reset_index().rename(columns={\"predicted\": \"predicted_qq_max\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54075afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_qq[(pred_qq.question == scores_avged.loc[0].question) & (pred_qq.faq_id == 85)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d3cc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_qq_maxed = scores_maxed.merge(pred_qq[['question', 'faq_id', 'actual']].drop_duplicates(), how=\"left\")\n",
    "pred_qq_maxed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdccebdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_qq_avged = scores_avged.merge(pred_qq[['question', 'faq_id', 'actual']].drop_duplicates(), how=\"left\")\n",
    "pred_qq_avged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee3e76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "248*104"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba4cdb5",
   "metadata": {},
   "source": [
    "# 3. Combine scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31584f60",
   "metadata": {},
   "source": [
    "## 3.1 Method 1\n",
    "\n",
    "1. Given $q'$, compute scores for all $a_i$'s using Q-A scorer\n",
    "2. From top $N$ $a_i$'s, say $A_N$, get corresponding questions $Q_N = \\{q_{ij}| \\forall j \\text{ and } i \\text{ s.t. } a_i \\in A_N\\}$\n",
    "3. Compute scores for each pair $(q', q_{ij}), q_{ij}\\in Q_N$, score $s_{ij}$ using Q-Q scorer\n",
    "4. $s_i = \\max_j(s_{ij})$\n",
    "5. Rank $a_i$'s by $s_i$'s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6bc0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10_by_qa = pred.sort_values(by='predicted', ascending=False).groupby('question').head(10).sort_values(by=['question', 'predicted'], ascending=False)\n",
    "top_10_by_qa.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50af07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10_by_qa.rename(columns={\"predicted\": \"predicted_qa\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3486493c",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10_by_qa = top_10_by_qa.merge(scores_maxed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b87713",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10_by_qa.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01faeacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def get_top_k_accuracy(df, variable):\n",
    "    ranking_accuracy = defaultdict(list)\n",
    "    top_n = [1, 3, 5, 7, 10]\n",
    "    for question, gdf in df.groupby(\"question\"):\n",
    "        _df = gdf.sort_values(by=variable, ascending=False)\n",
    "        for n in top_n:\n",
    "            ranking_accuracy[f\"top_{n}\"].append((_df[\"actual\"].iloc[:n] == 1.0).any())\n",
    "\n",
    "    ranking_acc_result = dict()\n",
    "    for k, v in ranking_accuracy.items():\n",
    "        ranking_acc_result[k] = pd.Series(v).mean()\n",
    "    \n",
    "    return pd.Series(ranking_acc_result).to_frame()\n",
    "\n",
    "get_top_k_accuracy(top_10_by_qa, 'predicted_qq_max') * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe85952c",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_top_k_accuracy(top_10_by_qa.merge(scores_avged), 'predicted_qq_avg') * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed40800",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_top_k_accuracy(pred, 'predicted') * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8192c863",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_top_k_accuracy(pred_qq_maxed, 'predicted_qq_max') * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba412cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_top_k_accuracy(pred_qq_avged, 'predicted_qq_avg') * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58b8036",
   "metadata": {},
   "source": [
    "## 3.2 Method 2\n",
    "1. Compute scores for all $(q', a_i)$ (using Q-A)\n",
    "2. Compute scores for all $(q', q_{ij})$ (using Q-Q)\n",
    "3. Pool scores for each $a_i$ and rank\n",
    "\n",
    "Pooling methods:\n",
    "* Average\n",
    "* Max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb3e1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_qq_maxed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326cac00",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bef5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.shape, pred_qq_maxed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e226ae7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_qa_qq_max_merged = pred.merge(pred_qq_maxed)\n",
    "assert pred_qa_qq_max_merged.shape[0] == pred.shape[0]\n",
    "pred_qa_qq_max_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c2193c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_qa_qq_max_merged.loc[:, \"pooled_avg\"] = pred_qa_qq_max_merged[['predicted', 'predicted_qq_max']].mean(axis=1)\n",
    "pred_qa_qq_max_merged.loc[:, \"pooled_max\"] = pred_qa_qq_max_merged[['predicted', 'predicted_qq_max']].max(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e446eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_top_k_accuracy(pred_qa_qq_max_merged, 'pooled_max') * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5927464a",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_top_k_accuracy(pred_qa_qq_max_merged, 'pooled_avg') * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2040f3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_qa_qq_avg_merged = pred.merge(pred_qq_avged)\n",
    "pred_qa_qq_avg_merged.loc[:, \"pooled_avg\"] = pred_qa_qq_avg_merged[['predicted', 'predicted_qq_avg']].mean(axis=1)\n",
    "pred_qa_qq_avg_merged.loc[:, \"pooled_max\"] = pred_qa_qq_avg_merged[['predicted', 'predicted_qq_avg']].max(axis=1)\n",
    "\n",
    "get_top_k_accuracy(pred_qa_qq_avg_merged, 'pooled_max') * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca97ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_top_k_accuracy(pred_qa_qq_avg_merged, 'pooled_avg') * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c3b319",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p38",
   "language": "python",
   "name": "conda_pytorch_p38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
